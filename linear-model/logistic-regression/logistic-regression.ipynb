{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(path, names=None):\n",
    "    return pd.read_csv(path, header=None, names=names)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = getData(os.getcwd() + \"/../data/ex2data1.txt\",[\"A\", \"B\", \"Y\"])\n",
    "data.insert(0,'ones',1)\n",
    "data['A'] = (data['A']-data['A'].min()) / (data['A'].max()-data['A'].min())\n",
    "data['B'] = (data['B']-data['B'].min()) / (data['B'].max()-data['B'].min())\n",
    "# 加入bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始实现逻辑回归， 因为在推导公式中，为了方便计算，类别是以，1,-1分类的， 因此需要将0换为-1.\n",
    "损失函数:$E_{in}(W)=\\sum\\limits_{n=1}^{N}ln(1+exp(-y_nW^Tx_n))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterData(data):\n",
    "    '''构造输入和标签'''\n",
    "    col = data.shape[1]\n",
    "    x = data.iloc[:,0:col-1]\n",
    "    y = data.iloc[:,col-1:col].replace(0, -1) \n",
    "    return np.array(x.values), np.array(y.values)\n",
    "\n",
    "def getWLin(X, Y):\n",
    "    '''线性回归计算初始值'''\n",
    "    return np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "\n",
    "def sigmod(s):\n",
    "    '''sigmod函数'''\n",
    "    return 1.0 / (1.0 + np.exp(-s))\n",
    "\n",
    "def update(W, grad, ita):\n",
    "    '''更新W'''\n",
    "    return W - (ita * grad)\n",
    "\n",
    "def MBGD(W, X, Y):\n",
    "    '''批量梯度下降'''\n",
    "    N = len(X)\n",
    "    grad = np.zeros(X[0].shape)\n",
    "    for i in range(N):\n",
    "        sig = sigmod(-Y[i][0] * X[i].dot(W))\n",
    "        grad += -Y[i][0] * sig * X[i]\n",
    "    \n",
    "    return grad / N\n",
    "\n",
    "def SGD(W, X, Y, n):\n",
    "    '''随机梯度'''\n",
    "    sig = sigmod(-Y[n][0] * X[n].dot(W))\n",
    "    return -Y[n][0] * sig * X[n]\n",
    "    \n",
    "    \n",
    "def cost(W, X, Y):\n",
    "    '''损失函数'''\n",
    "    N = len(X)\n",
    "    cost = 0.0\n",
    "    for i in range(N):\n",
    "        err = np.exp(-Y[i][0] * X[i].dot(W))\n",
    "        cost += np.log(1 + err)\n",
    "    \n",
    "    return cost / N\n",
    "\n",
    "def predict(W, X):\n",
    "    '''简单预测'''\n",
    "    predicts = [sigmod(x.dot(W)) for x in X]\n",
    "    return [1 if x >= 0.5 else 0 for x in predicts]\n",
    "\n",
    "def train_with_grad(X, Y, W, GD=1, iterations=30000):\n",
    "    '''训练'''\n",
    "    N = len(X)\n",
    "    for i in range(iterations):\n",
    "        if GD:\n",
    "            grad = MBGD(W, X, Y) # 全梯度\n",
    "        else:\n",
    "            grad = SGD(W, X, Y, i%N) # 随机梯度\n",
    "        err = cost(W, X, Y)\n",
    "        W = update(W, grad, 0.1)\n",
    "    return W, err\n",
    "    \n",
    "def train_with_fmin(W):\n",
    "    '''使用scipy的fmin优化'''\n",
    "    W = np.matrix(np.zeros(X[0].shape))\n",
    "    result = opt.fmin_tnc(func=cost, x0=W,fprime=MBGD, args=(X,Y))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = filterData(data)\n",
    "W = getWLin(X,Y).reshape(X[0].shape)\n",
    "W_SGD = train_with_grad(X, Y, W, GD=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = filterData(data)\n",
    "W = getWLin(X,Y).reshape(X[0].shape)\n",
    "W_MBGD = train_with_grad(X, Y, W, GD=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = getWLin(X,Y).reshape(X[0].shape)\n",
    "W_fmin = train_with_fmin(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([-11.07971084,  12.57405675,  12.07731836]), 0.20568539193828719),\n",
       " (array([-11.0468896 ,  12.47366075,  11.85883354]), 0.20547304306067637),\n",
       " (array([-12.81814377,  14.41152224,  13.77727802]), 36, 1))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_SGD, W_MBGD, W_fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20349796604639114"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(W_fmin[0], X, Y) # 计算fmin的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出误差已经很接近了，但是自己的优化函数的速度完全比不上使用第三方库，因此以后的优化问题均使用fmin。这个练习花了很长时间调正确，一个原因，不熟悉numpy的运算！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
